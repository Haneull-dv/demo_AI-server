import os
from dotenv import load_dotenv
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import logging

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

class ChatService:
    print(f"ğŸ©·3 ì„œë¹„ìŠ¤ ì§„ì…")
    def __init__(self):
        # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
        load_dotenv()
        self.hf_token = os.getenv("HUGGINGFACE_TOKEN")
        if not self.hf_token:
            raise ValueError("HUGGINGFACE_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.model_name = "lcw99/ko-dialoGPT-korean-chit-chat"
        logger.info(f"ëª¨ë¸ ë¡œë”© ì‹œì‘: {self.model_name}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            token=self.hf_token
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            token=self.hf_token
        )
        logger.info("ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

    async def chat(self, message: str):
        try:
            # ì…ë ¥ ë©”ì‹œì§€ ì „ì²˜ë¦¬
            input_text = f"User: {message}\nAssistant:"
            logger.info(f"ì…ë ¥ í…ìŠ¤íŠ¸: {input_text}")
            
            # ì…ë ¥ ë©”ì‹œì§€ í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer.encode(
                input_text,
                return_tensors='pt',
                add_special_tokens=True
            )
            
            # ì‘ë‹µ ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=200,  # ìµœëŒ€ ê¸¸ì´ ì¦ê°€
                    min_length=10,   # ìµœì†Œ ê¸¸ì´ ì„¤ì •
                    num_return_sequences=1,
                    no_repeat_ngram_size=3,
                    temperature=0.8,  # ì•½ê°„ ë” ì°½ì˜ì ì¸ ì‘ë‹µì„ ìœ„í•´ ì˜¨ë„ ìƒìŠ¹
                    top_k=50,
                    top_p=0.95,
                    do_sample=True,  # ìƒ˜í”Œë§ í™œì„±í™”
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.2  # ë°˜ë³µ íŒ¨ë„í‹° ì¶”ê°€
                )
            
            # ì‘ë‹µ ë””ì½”ë”©
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            logger.info(f"ì „ì²´ ì‘ë‹µ: {response}")
            
            # Assistant: ì´í›„ì˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
            if "Assistant:" in response:
                response = response.split("Assistant:")[-1].strip()
            else:
                response = response.replace(input_text, "").strip()
            
            logger.info(f"ìµœì¢… ì‘ë‹µ: {response}")
            
            if not response:
                response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            
            return {
                "response": response,
                "metadata": {
                    "model": self.model_name,
                    "temperature": 0.8
                }
            }
            
        except Exception as e:
            logger.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise Exception(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
